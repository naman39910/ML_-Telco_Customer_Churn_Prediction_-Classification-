{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "GF8Ens_Soomf",
        "PVzmfK_Ep1ck",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "tEA2Xm5dHt1r",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "Iwf50b-R2tYG",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "T5CmagL3EC8N",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "HAih1iBOpsJ2",
        "bmKjuQ-FpsJ3",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naman39910/ML_-Telco_Customer_Churn_Prediction_-Classification-/blob/main/ML__Telco_Customer_Churn_Prediction_(Classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Telco Customer Churn Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on analyzing the Telco Customer Churn dataset to build a machine learning model for predicting customer churn. The dataset contains comprehensive information about customers, including demographics (gender, SeniorCitizen, Partner, Dependents), account details (tenure, Contract, PaperlessBilling, PaymentMethod, MonthlyCharges, TotalCharges), and the services they subscribe to (PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies). The primary goal is to identify factors influencing churn and develop a predictive model to help the telecommunications company retain at-risk customers.\n",
        "\n",
        "The analysis began with data loading and initial exploration, revealing 7043 rows and 21 columns. The target variable, 'Churn', is binary, indicating whether a customer has left the company. Initial checks showed no duplicate entries or explicit missing values, although the 'TotalCharges' column, initially an object type, required conversion to numeric, which introduced 11 missing values. These were handled during the data wrangling phase.\n",
        "\n",
        "Data visualization played a crucial role in understanding the relationships between variables and their impact on churn. Bar charts and pie charts illustrated the overall churn rate and the distribution of categorical features. Histograms and KDE plots provided insights into the distribution of numerical features like 'tenure', 'MonthlyCharges', and 'TotalCharges' in relation to churn. Correlation heatmaps and pair plots helped in identifying relationships between numerical variables. Key insights from the visualizations included that customers with lower tenure, higher monthly charges, and certain service combinations (like Fiber Optic internet without online security or tech support) were more likely to churn.\n",
        "\n",
        "Hypothesis testing using t-tests, Chi-Square tests, and ANOVA confirmed some of these observations, showing statistically significant differences in 'MonthlyCharges' and 'Contract' type between churned and non-churned customers, while 'gender' did not show a significant association with churn.\n",
        "\n",
        "Data preprocessing involved handling the few missing values in 'TotalCharges' (though the provided code did not explicitly show imputation, the info() output indicates they are handled before modeling), and importantly, categorical encoding using One-Hot Encoding to convert the numerous object type features into a format suitable for machine learning models. The 'Churn' column was also encoded. Feature scaling using StandardScaler was applied to the numerical features to ensure they have a similar range.\n",
        "\n",
        "Several machine learning models were implemented and evaluated, including Random Forest, XGBoost, and K-Nearest Neighbors. Evaluation metrics such as accuracy, precision, recall, F1-score, and AUC-ROC were used, with a particular focus on metrics for the minority class (churned customers) to assess the models' ability to correctly identify customers likely to leave. Hyperparameter tuning using GridSearchCV was performed to optimize model performance and address potential overfitting observed in initial models like Random Forest. The XGBoost model, after tuning, demonstrated a good balance of performance across evaluation metrics, particularly in identifying churned customers, making it a strong candidate for the final prediction model. Feature importance analysis from models like XGBoost highlighted the most influential factors driving churn, such as contract type and internet service.\n",
        "\n",
        "In conclusion, this project successfully analyzed the Telco Customer Churn dataset, identified key drivers of churn through exploratory data analysis and statistical testing, preprocessed the data for machine learning, and developed predictive models. The XGBoost model, with its robust performance and ability to provide feature importance insights, offers a valuable tool for the telecommunications company to proactively identify and target at-risk customers with tailored retention strategies, ultimately contributing to reduced churn and improved business outcomes."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/naman39910/Exploratory-Data-Analysis/blob/main/ML__Telco_Customer_Churn_Prediction_(Classification).ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem statement for this project is to analyze the Telco Customer Churn dataset and build a machine learning model to predict which customers are likely to churn (leave the company).\n",
        "\n",
        "Telecommunications companies face the challenge of customer churn, which can significantly impact revenue and growth. Identifying customers at risk of churning is crucial for implementing targeted retention strategies.\n",
        "\n",
        "This project aims to:\n",
        "\n",
        "Understand the factors influencing customer churn: Analyze the various features in the dataset, including demographics, services subscribed to, and account information, to identify patterns and correlations with churn.\n",
        "Develop a predictive model: Build and evaluate machine learning models that can accurately predict the likelihood of a customer churning.\n",
        "Provide actionable insights: Based on the model's predictions and feature importance, provide insights to the telecommunications company on which customers are at risk and what factors are driving churn, enabling them to take proactive measures to retain those customers.\n",
        "Ultimately, the goal is to reduce customer churn and improve customer retention for the telecommunications company."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# 1. to handle the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# to visualize the data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# To preprocess the data\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder,OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "# import iterative imputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# machine learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "#for classification tasks\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "# pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "# metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error,mean_squared_error,r2_score, roc_auc_score\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dataset.duplicated().sum().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(dataset.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In This Dataset we find out 7043 Rows And 21 Columns .\n",
        "\n",
        "In This Dataset 2 Columns Are of Int datatype 1 Column Float Datatype And 18 Columns Are Object Datatype .\n",
        "\n",
        "By Using Info Function We Can See Count Of Columns And Rows DataType Of Data And Missing Values Also.In The Data 2 Columns Are of Int datatype And 1 Float Datatype And 18 Are Object Datatype ."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "dataset.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Converted the 0 and 1 value of SinorCitize into yes and no to amke it easier to read\n",
        "def conv(value):\n",
        "    if value == 0:\n",
        "        return 'No'\n",
        "    else:\n",
        "        return 'Yes'\n",
        "\n",
        "dataset['SeniorCitizen'] = dataset['SeniorCitizen'].apply(conv)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the customerid column\n",
        "dataset.drop('customerID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "KyStLOgAB37n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # The objective datatype conver into float datatype in TotalCharges columns\n",
        " dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')"
      ],
      "metadata": {
        "id": "RoTuOMBNCIOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "vGYHTPnLBv9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are manipulate some columns\n",
        "\n",
        "1.   A custom function conv is defined to convert the values in the SeniorCitizen column: it replaces 1 with 'Yes' and 0 with 'No' to make the data easier to interpret.\n",
        "2.   The customerID column, which is not useful for analysis, is dropped from the dataset using drop().\n",
        "3. The TotalCharges column, originally stored as an object (string) type, is converted to numeric (float) type using pd.to_numeric() .\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Bar Chart"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.countplot(x='Churn', data=dataset)\n",
        "plt.xlabel(\"Churn\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Churn Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use bar chart because A bar chart makes it easy to compare how many customers have churned (Yes) versus how many have not (No)."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At a glance, we can see that more customers stayed (No) than left (Yes). This is useful for understanding the imbalance in churn, which could be important for building predictive models."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even non-technical audiences can quickly understand the customer retention rate from this visual."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : Pie Chart"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "churn_counts = dataset['Churn'].value_counts()\n",
        "plt.pie(churn_counts, labels=churn_counts.index, autopct='%1.1f%%', startangle=90, colors=['green', 'red'])\n",
        "plt.title('Distribution of Churn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  The pie chart directly shows what percentage of customers churned (Yes) versus stayed (No).\n",
        "*   It gives a clear visual impression of how much bigger one group is compared to the other.\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Majority of Customers Did Not Churn: 73.5% of the customers did not leave the company. This indicates a high customer retention rate, which is a positive sign for the business.\n",
        "*   About 1 in 4 Customers Churned:\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Losing more than 1 in every 4 customers is a potential revenue and growth concern. It suggests the company should investigate why customers are leaving and improve retention strategies."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Histogram Chart"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "churned = dataset[dataset['Churn'] == 'Yes']\n",
        "not_churned = dataset[dataset['Churn'] == 'No']\n",
        "\n",
        "plt.hist([churned['tenure'], not_churned['tenure']], bins= 10,\n",
        "         color =['red', 'blue'], label = ['yes', 'no'])\n",
        "plt.title('Distribution of Tenure by Churn Status')\n",
        "plt.xlabel('Tenure')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because It clearly compares the number of customers who churned (yes) vs. those who stayed (no) at different tenure levels."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some insides found in this chart\n",
        "\n",
        "\n",
        "1.   Most churn happens early in the customer lifecycle\n",
        "2.   Customers with longer tenure tend to stay\n",
        "\n",
        "1.   Sharp contrast at the extremes\n",
        "2.   Churn decreases consistently over time\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduce loyalty rewards or long-term contracts to encourage customers to stay past the early-risk period."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : Histogram Plot"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# make plot for MonthlyCharges\n",
        "churned = dataset[dataset['Churn'] == 'Yes']\n",
        "not_churned = dataset[dataset['Churn'] == 'No']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist([churned['MonthlyCharges'], not_churned['MonthlyCharges']], bins= 10,\n",
        "         color =['red', 'blue'], label = ['yes', 'no'])\n",
        "plt.title('Distribution of Monthly Charges by Churn Status')\n",
        "plt.xlabel('Monthly Charges')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because  It directly relates churn to pricing (a key business metric)\n",
        "we do Clear comparison between churned (red) and retained (blue) customers ."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Customers with low monthly charges (~$20–30) are mostly retained.\n",
        "*   Churn increases sharply in the $70–100 range — possibly due to dissatisfaction with perceived value at higher prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Target high-charging customers (e.g., $70+) with retention offers, loyalty rewards, or better service.\n",
        "*   Reduce churn = more revenue continuity.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : Histogram Plot"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "churned = dataset[dataset['Churn'] == 'Yes']\n",
        "not_churned = dataset[dataset['Churn'] == 'No']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist([churned['TotalCharges'], not_churned['TotalCharges']], bins= 10,\n",
        "         color =['red', 'blue'], label = ['yes', 'no'])\n",
        "plt.title('Distribution of Total Charges by Churn Status')\n",
        "plt.xlabel('Total Charges')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Easy to read\n",
        "\n",
        "Ideal for comparing distributions\n",
        "\n",
        "Helpful for churn pattern discovery based on financial contribution (Total Charges)"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We founds some inside in this chart\n",
        "\n",
        "\n",
        "1.   Customers with lower Total Charges are more likely to churn\n",
        "\n",
        "1.   Long-term customers (higher Total Charges) have lower churn\n",
        "2.   Churn rate declines as Total Charges increase\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers who churn typically have low Total Charges, indicating they leave early. This highlights the need to improve early customer experience (onboarding, initial billing, value perception) to reduce churn."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 : kde plot"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "# Plot for Monthly Charges\n",
        "sns.kdeplot(data=dataset, x=\"MonthlyCharges\", hue=\"Churn\", fill=True, alpha=0.5, ax=axes[0])\n",
        "axes[0].set_title('Density Plot of Monthly Charges by Churn Status')\n",
        "axes[0].set_xlabel('Monthly Charges')\n",
        "axes[0].set_ylabel('Density')\n",
        "\n",
        "# Plot for Total Charges\n",
        "sns.kdeplot(data=dataset, x=\"TotalCharges\", hue=\"Churn\", fill=True, alpha=0.5, ax=axes[1])\n",
        "axes[1].set_title('Density Plot of Total Charges by Churn Status')\n",
        "axes[1].set_xlabel('Total Charges')\n",
        "axes[1].set_ylabel('Density')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KDE (Kernel Density Estimation) plots smooth the distribution and make it easy to compare the shape and spread of values for churned (Yes) vs. non-churned (No) customers."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Customers with higher monthly costs are more likely to churn, possibly due to perceived lack of value or affordability issues.\n",
        "*   Churned customers don’t stay long enough to generate high total revenue, which hurts long-term profitability.\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : Line Chart"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(x='tenure', y='MonthlyCharges', data=dataset)\n",
        "plt.title('Monthly Charges Over Time')\n",
        "plt.xlabel('Tenure')\n",
        "plt.ylabel('Monthly Charges')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Line plots are ideal for showing how a numerical variable (MonthlyCharges) changes across a continuous variable (tenure).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we found some insides in this chart\n",
        "\n",
        "\n",
        "1.   Monthly Charges Increase with Tenure (Slightly) .\n",
        "2.   High Variation in Monthly Charges at All Tenure Levels .\n",
        "3. Stabilization with Spikes After ~10 Months .\n",
        "\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long-term customers tend to spend more monthly, so focusing on retention and upselling can increase revenue."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 : Bar Chart"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Define the colors for yes or no\n",
        "colors = ['blue','red']\n",
        "\n",
        "# add missing key for 0 and 1 in the palette\n",
        "palette = {0 :'blue', 1: 'red'}\n",
        "\n",
        "for i, predictor in enumerate(dataset.drop(columns=['Churn', 'TotalCharges', 'MonthlyCharges', 'tenure'])):\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    sns.countplot(data=dataset, x=predictor, hue='Churn', palette=colors)\n",
        "    plt.title(predictor)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables like gender, SeniorCitizen, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, etc., are categorical. so use countplot because it perfect for visualizing how many observations fall into each category, and also works well with hue='Churn' to split by churn status."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that some insides in these charts\n",
        "\n",
        "\n",
        "*   Senior Citizens are more likely to churn\n",
        "\n",
        "*   Customers without partners or dependents churn more\n",
        "*   No tech-related services → higher churn\n",
        "\n",
        "\n",
        "*   Month-to-month contracts have the highest churn\n",
        "\n",
        "\n",
        "*   Paperless billing slightly linked to higher churn\n",
        "*   Fiber optic internet users churn more\n",
        "\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we change some stratigy to create a posive impact in business\n",
        "1.   Targeted Retention Campaigns\n",
        "Who to target: Senior citizens, month-to-month contract users, and those without partners or dependents.\n",
        "\n",
        "Impact: Tailored offers, better customer support, or loyalty benefits can reduce churn in these high-risk segments.\n",
        "2.    Contract Strategy\n",
        "Insight: Customers with long-term contracts churn less.\n",
        "\n",
        "Action: Promote yearly contracts with discounts or perks to lock in loyalty.\n",
        "\n",
        "3. Product & Service Improvements\n",
        "Customers without tech support or online security churn more.\n",
        "\n",
        "Action: Improve or bundle these services to increase perceived value and reduce churn.\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "numeric_dataset = dataset.select_dtypes(include=np.number)\n",
        "sns.heatmap(numeric_dataset.corr(), annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Quickly See Relationships Between Variables: A heatmap visually shows how strongly related different numeric variables are, using both color intensity and numeric correlation values (Pearson correlation coefficients).\n",
        "*   Simplifies Complexity: When dealing with many variables, heatmaps help summarize relationships in one compact visual instead of checking each scatter plot manually.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Strong Positive Correlation: tenure vs. TotalCharges (0.83)\n",
        "\n",
        "1.   Moderate Positive Correlation: MonthlyCharges vs. TotalCharges (0.65)\n",
        "2.   Weak Correlation: tenure vs. MonthlyCharges (0.25)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "numeric_dataset = dataset.select_dtypes(include=np.number)\n",
        "sns.pairplot(numeric_dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multivariable Exploration: This pair plot shows relationships between tenure, MonthlyCharges, and TotalCharges — all important numerical features in telecom churn analysis."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some insides found in this chart\n",
        "\n",
        "\n",
        "1. Strong Positive Correlation: TotalCharges vs. tenure (As tenure increases, TotalCharges also increase.)\n",
        "2.   Positive Correlation: TotalCharges vs. MonthlyCharges (Customers with higher monthly charges generally have higher total charges.)\n",
        "\n",
        "\n",
        "3.   Weak/No Clear Correlation: tenure vs. MonthlyCharges (There's no strong trend between tenure and monthly charges.)  "
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 t-test"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): μ\n",
        "churned\n",
        "​\n",
        " =μ\n",
        "not churned\n",
        "​\n",
        "\n",
        "Alternative Hypothesis (H₁): μ\n",
        "churned\n",
        "​ != μ\n",
        "not churned\n",
        "​\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Clean and convert MonthlyCharges to numeric, if needed\n",
        "dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing MonthlyCharges or Churn\n",
        "dataset = dataset.dropna(subset=['MonthlyCharges', 'Churn'])\n",
        "\n",
        "# Separate the MonthlyCharges based on Churn values\n",
        "churn_yes = dataset[dataset['Churn'] == 'Yes']['MonthlyCharges']\n",
        "churn_no = dataset[dataset['Churn'] == 'No']['MonthlyCharges']\n",
        "\n",
        "# Import ttest_ind\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Perform independent t-test (Welch's t-test)\n",
        "t_stat, p_value = ttest_ind(churn_yes, churn_no, equal_var=False)\n",
        "\n",
        "t_stat, p_value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical testing to obtain P-Value and found the result that Null hypothesis has been rejected .\n",
        "There is a statistically significant difference in average MonthlyCharges between customers who churned and those who did not."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-test is used in case to compare whether the average MonthlyCharges differs significantly between customers who churned and those who did not. Here's a full explanation ."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 Chi-Square Test"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no association between Churn and gender. (They are independent.)\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "There is an association between Churn and gender."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "# Create a contingency table for Churn vs Gender\n",
        "contingency_table = pd.crosstab(dataset['Churn'], dataset['gender'])\n",
        "\n",
        "# Perform Chi-Square Test of Independence\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "chi2_stat, p_value, dof, expected\n",
        "# Output the results\n",
        "print(\"Chi-square Statistic:\", chi2_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"Expected Frequencies Table:\\n\", expected)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used chi-square Test as the statistical testing to obtain P-Value and found the result that p-value < 0.05: Reject the null → Churn is dependent on gender."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-Square Test of Independence is chosen when i want to examine whether two categorical variables are associated (i.e., dependent) or not associated (i.e., independent)."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 ANOVA test"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "Mean MonthlyCharges are equal across all contract types.\n",
        "\n",
        "𝜇\n",
        "1\n",
        "=\n",
        "𝜇\n",
        "2\n",
        "=\n",
        "𝜇\n",
        "3\n",
        "μ\n",
        "\n",
        "\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "At least one group's mean MonthlyCharges is different."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "# Drop rows with missing MonthlyCharges or Contract values\n",
        "dataset = dataset.dropna(subset=['MonthlyCharges', 'Contract'])\n",
        "\n",
        "# Group MonthlyCharges by Contract type\n",
        "month_to_month = dataset[dataset['Contract'] == 'Month-to-month']['MonthlyCharges']\n",
        "one_year = dataset[dataset['Contract'] == 'One year']['MonthlyCharges']\n",
        "two_year = dataset[dataset['Contract'] == 'Two year']['MonthlyCharges']\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value = f_oneway(month_to_month, one_year, two_year)\n",
        "\n",
        "# Print results\n",
        "print(\"F-statistic:\", f_stat)\n",
        "print(\"p-value:\", p_value)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used ANOVATest as the statistical testing to obtain P-Value and found the result that p-value ≥ 0.05 → Fail to reject H₀ → No evidence of significant difference .\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ANOVA (Analysis of Variance) test is used when you want to compare the means of a continuous variable across more than two groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "dataset.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing value in this dataset ."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments.\n",
        "sns.boxplot(dataset)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no significant outliers in the numerical columns of the dataset."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Identify the categorical columns\n",
        "categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Apply OneHotEncoding\n",
        "encoder = OneHotEncoder(sparse_output=False, drop=None, handle_unknown='ignore')\n",
        "encoded_data = encoder.fit_transform(dataset[categorical_columns])\n",
        "\n",
        "# Create a new DataFrame with the encoded data\n",
        "encoded_df = pd.DataFrame(encoded_data,\n",
        "                          columns=encoder.get_feature_names_out(categorical_columns),\n",
        "                          index=dataset.index)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "dataset.drop(columns=categorical_columns, inplace=True)\n",
        "\n",
        "# Concatenate the original DataFrame with the encoded DataFrame\n",
        "dataset = pd.concat([dataset, encoded_df], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "zeHsyFBVt_Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.drop('Churn_No', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "fUzTWLiXuiRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "Hl1HmJE9uoQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding converts each categorical column into multiple binary (0/1) columns—one for each category level.\n",
        "\n",
        "One-Hot Encoding was used in this code:\n",
        "1. Suitable for nominal categorical data (i.e., unordered categories like gender, city, product type).\n",
        "One-hot encoding treats all categories as equally important without assuming any order or ranking.\n",
        "\n",
        "2. Required for many ML models (e.g., Linear Regression, Logistic Regression, SVM, etc.)\n",
        "These models cannot handle string or label-type categorical values directly, so encoding is necessary.\n",
        "\n",
        "3. Scikit-learn compatibility:\n",
        "You used sklearn.preprocessing.OneHotEncoder which is a standard and robust way to convert categorical features.\n",
        "\n",
        "4. Avoids introducing bias:\n",
        "Unlike Label Encoding, which can create a false sense of order (e.g., assigning Male = 0, Female = 1), One-Hot does not assume any order."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "There are no text columns in the given dataset which I am working on. So, Skipping this part."
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "sns.scatterplot(x='tenure', y='TotalCharges', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# rename churn_yes to churn\n",
        "dataset.rename(columns={'Churn_Yes': 'Churn'}, inplace=True)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [i for i in dataset.columns if i not in ['Churn']]"
      ],
      "metadata": {
        "id": "CWoaY-rusETw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(dataset[features])"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "In this i have different independent features of different scale so i have used standard scalar method to scale our independent features into one scale."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset dimesionality not required."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# split data into X and y\n",
        "X = dataset.drop('Churn', axis=1)\n",
        "y = dataset['Churn']\n",
        "# data into train and split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset, we've used a data splitting ratio of 80:20, meaning:\n",
        "\n",
        "\n",
        "*   80% of the data is used for training (X_train, y_train)\n",
        "*   20% of the data is used for testing (X_test, y_test)\n",
        "\n",
        "\n",
        "Why this 80:20 ratio is used:\n",
        "\n",
        "\n",
        "1.   Balanced Trade-off:\n",
        "\n",
        "\n",
        "*   80% training data allows the model to learn well from a large portion of the dataset.\n",
        "*   20% testing data is sufficient to evaluate the model’s performance on unseen data.\n",
        "\n",
        "2.   Industry Standard:\n",
        "\n",
        "\n",
        "*   This is a commonly used split in machine learning as a good starting point, especially when dataset size is reasonable.\n",
        "\n",
        "3. Prevents Overfitting and Underfitting:\n",
        "\n",
        "*   Enough data for training reduces underfitting.\n",
        "*   A separate test set helps in detecting overfitting by checking generalization on new\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.Churn.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "dataset['Churn'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Not Churn(%)','Churn(%)'],\n",
        "                               colors=['skyblue','red'],\n",
        "                               explode=[0,0]\n",
        "                              )"
      ],
      "metadata": {
        "id": "8wz32LKoRK5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used SMOTE (Synthetic Minority Over-sampling technique) for balanced the 85:15 dataset.\n",
        "\n",
        "SMOTE is a technique in machine learning for dealing with issues that arise when working with an unbalanced data set. In practice, unbalanced data sets are common and most ML algorithms are highly prone to unbalanced data so we need to improve their performance by using techniques like SMOTE.\n",
        "\n",
        "To address this disparity, balancing schemes that augment the data to make it more balanced before training the classifier were proposed. Oversampling the minority class by duplicating minority samples or undersampling the majority class is the simplest balancing method.\n",
        "\n",
        "The idea of incorporating synthetic minority samples into tabular data was first proposed in SMOTE, where synthetic minority samples are generated by interpolating pairs of original minority points.\n",
        "\n",
        "SMOTE is a data augmentation algorithm that creates synthetic data points from raw data. SMOTE can be thought of as a more sophisticated version of oversampling or a specific data augmentation algorithm.\n",
        "\n",
        "SMOTE has the advantage of not creating duplicate data points, but rather synthetic data points that differ slightly from the original data points. SMOTE is a superior oversampling option.\n",
        "\n",
        "That's why for lots of advantages, I have used SMOTE technique for balancinmg the dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to print evaluation matrix\n",
        "def evaluate_model(model, y_test, y_pred):\n",
        "\n",
        "  '''takes model, y test and y pred values to print evaluation metrics, plot the actual and predicted values,\n",
        "  plot the top 20 important features, and returns a list of the model scores'''\n",
        "\n",
        "  # Squring the y test and and pred as we have used sqrt transformation\n",
        "  y_t = np.square(y_test)\n",
        "  y_p = np.square(y_pred)\n",
        "  y_train2 = np.square(y_train)\n",
        "  y_train_pred = np.square(model.predict(X_train))\n",
        "\n",
        "  # Calculating Evaluation Matrix\n",
        "  mse = mean_squared_error(y_t,y_p)\n",
        "  rmse = np.sqrt(mse)\n",
        "  mae = mean_absolute_error(y_t,y_p)\n",
        "  r2_train = r2_score(y_train2, y_train_pred)\n",
        "  r2 = r2_score(y_t,y_p)\n",
        "  r2_adjusted = 1-(1-r2)*((len(X_test)-1)/(len(X_test)-X_test.shape[1]-1))\n",
        "\n",
        "  # Printing Evaluation Matrix\n",
        "  print(\"MSE :\" , mse)\n",
        "  print(\"RMSE :\" ,rmse)\n",
        "  print(\"MAE :\" ,mae)\n",
        "  print(\"Train R2 :\" ,r2_train)\n",
        "  print(\"Test R2 :\" ,r2)\n",
        "  print(\"Adjusted R2 : \", r2_adjusted)\n",
        "\n",
        "\n",
        "  # plot actual and predicted values\n",
        "  plt.figure(figsize=(13,4))\n",
        "  plt.plot((y_p)[:100])\n",
        "  plt.plot((np.array(y_t)[:100]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.title('Actual and Predicted Bike Count', fontsize=15)\n",
        "\n",
        "  try:\n",
        "    importance = model.feature_importances_\n",
        "  except:\n",
        "    importance = model.coef_\n",
        "  importance = np.absolute(importance)\n",
        "  if len(importance)==len(features):\n",
        "    pass\n",
        "  else:\n",
        "    importance = importance[0]\n",
        "\n",
        "  # Feature importances\n",
        "  feat = pd.Series(importance, index=features)\n",
        "  plt.figure(figsize=(9,7))\n",
        "  plt.title('Feature Importances (top 20) for '+str(model), fontsize = 15)\n",
        "  plt.xlabel('Relative Importance')\n",
        "  feat.nlargest(20).plot(kind='barh')\n",
        "\n",
        "\n",
        "  model_score = [mse,rmse,mae,r2_train,r2,r2_adjusted]\n",
        "  return model_score"
      ],
      "metadata": {
        "id": "gWUc6Q-fVQsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a score dataframe\n",
        "score = pd.DataFrame(index = ['MSE', 'RMSE', 'MAE', 'Train R2', 'Test R2', 'Adjusted R2'])"
      ],
      "metadata": {
        "id": "O7VBNAFKVbqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 : RandomFores"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_class_preds = rf_model.predict(X_train)\n",
        "test_class_preds = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "ULgBUAmTpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy on train and test\n",
        "train_accuracy = accuracy_score(y_train,train_class_preds)\n",
        "test_accuracy = accuracy_score(y_test,test_class_preds)\n",
        "\n",
        "print(\"The accuracy on train dataset is\", train_accuracy)\n",
        "print(\"The accuracy on test dataset is\", test_accuracy)"
      ],
      "metadata": {
        "id": "LFlq7TTry1WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "AfpcadjL0jT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(metrics.classification_report(y_true=y_train, y_pred=train_class_preds))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_true=y_train, y_score=train_class_preds))"
      ],
      "metadata": {
        "id": "2glUq3Ve0rit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "metadata": {
        "id": "XvvRXbfb8okp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I used Random Forest algorithm to create the model. As I got there is overfitting seen.\n",
        "\n",
        "For training dataset, i found precision of 100% and recall of 99% and f1-score of 100% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 100% and recall of 100% and f1-score of 100%. Accuracy is 99% and average percision, recall & f1_score are 100%, 100% and 99% respectively with a roc auc score of 99%.\n",
        "\n",
        "For testing dataset, i found precision of 91% and recall of 82% and f1-score of 86% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 46% and recall of 65% and f1-score of 53%. Accuracy is 78% and average percision, recall & f1_score are 68%, 73% and 70% respectively with a roc auc score of 68%.\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "wgqF-TQ89Vkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X.columns), # Use X.columns to get the feature names\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "krTwECt1_qe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)\n",
        "features = X.columns\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "oNo0U6go_19E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9LPuktmAF4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use StratifiedKFold for balanced CV\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    rf, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"✅ Best Parameters:\", grid_search.best_params_)\n",
        "print(\"🎯 Accuracy on Test Set:\", accuracy_score(y_test, y_pred))\n",
        "print(\"📄 Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "dSAifb_g7Ok8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train Random Forest with best parameters (or default for demo)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = rf.predict(X_train)\n",
        "test_preds = rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Train Accuracy:\", accuracy_score(y_train, train_preds))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))\n",
        "\n",
        "# Confusion Matrix\n",
        "train_cm = confusion_matrix(y_train, train_preds)\n",
        "test_cm = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "# 🔲 Plot confusion matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title('Train Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
        "axes[1].set_title('Test Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 📊 Plot classification report for test set\n",
        "report = classification_report(y_test, test_preds, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose().drop(\"accuracy\")\n",
        "\n",
        "report_df[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(10, 6))\n",
        "plt.title(\"Test Set Classification Metrics\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ptaLoselNG7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))"
      ],
      "metadata": {
        "id": "fR3jW3q9NoFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypertuned Random Forest\n",
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "metadata": {
        "id": "AaqbYST7NxP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, i found precision of 100% and recall of 83% and f1-score of 87% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 47% and recall of 65% and f1-score of 54%. Accuracy is 91% and average percision, recall & f1_score are 69%, 74% and 71% respectively with a roc auc score of 56%.\n",
        "\n",
        "Quite improvment seen as no overfitting but the scores reduced by some percentages.\n",
        "\n",
        "For testing dataset, i found precision of 91% and recall of 83% and f1-score of 87% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 13% and recall of 90% and f1-score of 23%. Accuracy is 91% and average percision, recall & f1_score are 69%, 74% and 71% respectively with a roc auc score of 56%.\n",
        "\n",
        "Quite improvemnt seen in recall but rest scores have decreased."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 : XG Boost"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "xg_model = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xg_models=xg_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "train_class_preds = xg_models.predict(X_train)\n",
        "test_class_preds = xg_models.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "Vxm1aedwMDaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))"
      ],
      "metadata": {
        "id": "NCfJ_YchMHmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "metadata": {
        "id": "IeJUS58OMMF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = xg_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "xEHb-DIPOlou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X.columns\n",
        "importances = xg_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "r9f46bXQPjvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mC0EP2K1PuNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define XGBoost model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=xgb,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "FbiBrPj8QYuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict on the training and testing sets using the best model\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# Confusion matrix - Train\n",
        "cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=best_model.classes_)\n",
        "disp_train.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Train Set\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix - Test\n",
        "cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=best_model.classes_)\n",
        "disp_test.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "report_train = classification_report(y_train, y_train_pred, output_dict=True)\n",
        "report_test = classification_report(y_test, y_test_pred, output_dict=True)\n",
        "\n",
        "# Extract metrics\n",
        "metrics_to_plot = ['precision', 'recall', 'f1-score']\n",
        "labels = ['Train', 'Test']\n",
        "classes = ['0', '1']  # Class labels (No churn, Churn)\n",
        "\n",
        "# Plot each metric for class 1 (Churn)\n",
        "for metric in metrics_to_plot:\n",
        "    values = [report_train['1.0'][metric], report_test['1.0'][metric]]\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.barplot(x=labels, y=values, palette='pastel')\n",
        "    plt.title(f\"{metric.title()} for Class 1 (Churn)\")\n",
        "    plt.ylabel(metric.title())\n",
        "    plt.ylim(0, 1)\n",
        "    for i, v in enumerate(values):\n",
        "        plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "o0rtqiw1Sw4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))"
      ],
      "metadata": {
        "id": "JllBmQSmUFF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, i found precision of 97% and recall of 95% and f1-score of 96% for False Churn customer data. and roc auc score of 91%."
      ],
      "metadata": {
        "id": "SygOxI6cVO2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "metadata": {
        "id": "oQs9EFB9UIot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For testing dataset, i found precision of 89% and recall of 84% and f1-score of 86% for False Churn customer data. and roc auc score is  70% ."
      ],
      "metadata": {
        "id": "w5i_XdhwVsXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 : KNeighbors"
      ],
      "metadata": {
        "id": "4wTKw10fahFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "# Create an instance of the KNeighbors\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "knn_models=knn_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "train_class_preds = knn_models.predict(X_train)\n",
        "test_class_preds = knn_models.predict(X_test)"
      ],
      "metadata": {
        "id": "VlNlbrEUcBde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "SiG7pD-tfB9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "xiGDK_TDcjEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "vsOUDW3rdBt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))"
      ],
      "metadata": {
        "id": "v0ekrumGdSFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, i found precision of 92% and recall of 85% and f1-score of 89% for False Churn customer data. and roc auc score of 74%."
      ],
      "metadata": {
        "id": "lKbE6d1wgD-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "metadata": {
        "id": "oSg8mxcXdUZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For testing dataset, i found precision of 88% and recall of 83% and f1-score of 85% for False Churn customer data. and roc auc score of 68%."
      ],
      "metadata": {
        "id": "3_MjbCpzf3Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DM-BoZMNf1_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "PpHFeGfEbqXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {'n_neighbors': list(range(3, 16)), 'weights': ['uniform', 'distance']}\n",
        "knn = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_knn = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = best_knn.predict(X_train)\n",
        "y_test_pred = best_knn.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "-zCJnj8AaTjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and classification report\n",
        "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "XiTwqPABbBRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrices\n",
        "cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "ConfusionMatrixDisplay(cm_train).plot(cmap='Blues')\n",
        "plt.title(\"Train Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ngbMAQG6bGRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "ConfusionMatrixDisplay(cm_test).plot(cmap='Blues')\n",
        "plt.title(\"Test Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Izmf0SCFbNA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize evaluation metrics\n",
        "report_test = classification_report(y_test, y_test_pred, output_dict=True)\n",
        "metrics_to_plot = ['precision', 'recall', 'f1-score']\n",
        "for metric in metrics_to_plot:\n",
        "    value = report_test['1.0'][metric]\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.barplot(x=['Class 1 (Churn)'], y=[value], palette='mako')\n",
        "    plt.title(f\"{metric.title()} - Class 1 (Churn)\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.text(0, value + 0.02, f\"{value:.2f}\", ha='center')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "P67jM30abReA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate models for positive business impact in the Telco Customer Churn problem, I considered the following evaluation metrics, not just for technical performance but also for their strategic business value:\n",
        "\n",
        "1. F1-Score (Especially for Class 1 - Churn)\n",
        "Why it matters:\n",
        "Churned customers (class = 1) are the minority class and most critical to identify.\n",
        "\n",
        "Business impact:\n",
        "High F1-score means fewer missed churners (false negatives) and fewer false alarms (false positives).\n",
        "\n",
        "Balanced view:\n",
        "Combines precision (how many predicted churns actually churned) and recall (how many actual churns we captured).\n",
        "\n",
        "Chosen as primary metric because:\n",
        "\n",
        "❝ Missing a true churner (FN) is costlier than targeting a wrong customer (FP) ❞\n",
        "\n",
        "\n",
        "2. Recall (Sensitivity) – Focused on Churned Customers\n",
        "Why it matters:\n",
        "Recall = TP / (TP + FN) — how many actual churners were identified?\n",
        "\n",
        "Business impact:\n",
        "High recall = fewer lost customers = more retention = direct revenue gain.\n",
        "\n",
        "Chosen because:\n",
        "\n",
        "❝ It's better to flag more potential churners and apply retention offers than to miss them entirely ❞\n",
        "\n",
        "3. Precision – Also for Churned Class\n",
        "Why it matters:\n",
        "How many of the predicted churns were actually churners?\n",
        "\n",
        "Business impact:\n",
        "High precision = fewer wasted resources (offers, discounts, calls) on customers who were not going to churn.\n",
        "\n",
        "Chosen to optimize operational cost of churn-prevention efforts.\n",
        "\n",
        "4. Confusion Matrix\n",
        "Why it matters:\n",
        "Gives raw insight into:\n",
        "\n",
        "How many customers we saved (True Positives)\n",
        "\n",
        "How many we missed (False Negatives)\n",
        "\n",
        "Business impact:\n",
        "Directly supports cost-benefit analysis of the model's output.\n",
        "\n",
        "5. Accuracy (used carefully)\n",
        "Why it matters:\n",
        "Gives an overall sense of model performance.\n",
        "\n",
        "Caution:\n",
        "Can be misleading in imbalanced datasets like churn (e.g., 80% non-churn = 80% accuracy even if we predict all as \"No\").\n",
        "\n",
        "Not the primary metric, but useful when paired with others."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the models we explored — including KNeighborsClassifier (KNN) and XGBClassifier — the XGBoost model was selected as the final prediction model for Telco Customer Churn prediction.\n",
        "\n",
        "Here’s why:\n",
        " 1. Superior Performance on Key Metrics ->\n",
        "\n",
        " In churn prediction, catching as many actual churners as possible (recall) is crucial for retaining revenue. XGBoost consistently delivered better recall and F1-score for the positive class (Churn).\n",
        "\n",
        "2. Scalability & Speed ->\n",
        "\n",
        "    XGBoost is highly optimized for large datasets.\n",
        "\n",
        "    Much faster at inference than KNN (which needs to compute distances to all training points).\n",
        "\n",
        "    Suitable for real-time prediction systems.\n",
        "\n",
        "3. Robust to Imbalanced Data ->\n",
        "\n",
        "     XGBoost supports built-in class weighting (scale_pos_weight) which helps handle class imbalance (more “No Churn” than “Yes”).\n",
        "\n",
        "    KNN lacks this built-in balancing, leading to biased results.\n",
        "\n",
        "4. Explainability for Business Decisions ->\n",
        "\n",
        "    XGBoost provides:\n",
        "\n",
        "            Feature importance scores\n",
        "\n",
        "            SHAP values (advanced interpretability)\n",
        "\n",
        "    Helps business users understand \"why\" a customer may churn, making it easier to justify retention actions.\n",
        "\n",
        "5. Hyperparameter Tuning Results ->\n",
        "\n",
        "    After applying GridSearchCV, XGBoost consistently outperformed KNN on the test set.\n",
        "\n",
        "    Tuned parameters like n_estimators, max_depth, and learning_rate gave even better predictive power.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a complete explanation of the final model (XGBoost) and its feature importance using model explainability tools, specifically:\n",
        "\n",
        "1. Feature Importance with XGBoost ->\n",
        "After training, XGBoost allows us to extract and visualize important features. These tell us which factors most influence churn prediction.\n",
        "\n",
        "2. Advanced Explainability with SHAP\n",
        "🔹 What is SHAP?\n",
        "SHAP (SHapley Additive exPlanations) explains the impact of each feature on a specific prediction.\n",
        "It answers:\n",
        "“Which features pushed this prediction toward churn or non-churn, and by how much?”\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The XGBoost model provides a highly accurate, interpretable, and business-aligned solution for churn prediction.\n",
        "\n",
        "*  It enables the company to proactively retain customers, reduce revenue loss, and build better service strategies using data-driven decisions.\n",
        "\n",
        "\n",
        "*   Low tenure and high monthly charges are key churn drivers.\n",
        "\n",
        "*   Long-term contracts and tech support reduce churn.\n",
        "*   This helps design personalized retention strategies (e.g., offer better plans to high-risk users).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}